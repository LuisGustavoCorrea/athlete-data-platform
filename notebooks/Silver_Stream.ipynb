{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d67c098-2392-439f-800b-e99c95771b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "from delta.tables import DeltaTable\n",
    "from py_functions_silver import *\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from dataquality_rules_strava import get_rules_activity\n",
    "\n",
    "\n",
    "BRONZE = \"uc_athlete_data.bronze.strava_activities\"\n",
    "SILVER = \"uc_athlete_data.silver.strava_activities\"\n",
    "CHECKPOINT = \"abfss://silver@adlsathlete.dfs.core.windows.net/strava/activities/activities_checkpoint/\"\n",
    "REJECT_TABLE = \"uc_athlete_data.silver_rejects.strava_activities\"\n",
    "\n",
    "# chave(s) de negócio — mude p/ composto se precisar, ex.: [\"activity_id\",\"lap_index\"]\n",
    "BUSINESS_KEYS = [\"id\"]\n",
    "# colunas de ordenação para decidir o “vencedor” nos duplicados do micro-lote\n",
    "ORDER_COLS = [\"ingestion_timestamp\"]  # adicione \"updated_at\" se existir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd7c8014-0e43-497c-abb7-a111a5dd5ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def assert_quality(df):\n",
    "    # 1) Defina as regras (nome -> condição SQL)\n",
    "    rules = get_rules_activity()\n",
    "    \n",
    "    # 2) Constrói uma coluna array com TODOS os motivos que baterem\n",
    "    #    (evita múltiplos scans/union e duplicaçāo na rejects)\n",
    "    #dfq = df.withColumn(\"reject_reasons\", F.array())  # array<string> inicial\n",
    "    dfq = df.withColumn(\"reject_reasons\", F.lit(None).cast(\"array<string>\"))\n",
    "    \n",
    "    for name, sql_cond in rules.items():\n",
    "        cond_col = F.expr(sql_cond)\n",
    "        dfq = dfq.withColumn(\n",
    "            \"reject_reasons\",\n",
    "            F.when(\n",
    "                cond_col,\n",
    "                # coalesce para lidar com NULL -> vira [] antes de unir com [name]\n",
    "                F.array_union(F.coalesce(F.col(\"reject_reasons\"), F.array()), F.array(F.lit(name)))\n",
    "            ).otherwise(F.col(\"reject_reasons\"))\n",
    "        )\n",
    "\n",
    "    # 3) Flag, contador e string consolidada (para leitura humana)\n",
    "    dfq = (dfq\n",
    "           .withColumn(\"reject_reason_count\",\n",
    "                       F.when(F.col(\"reject_reasons\").isNull(), F.lit(0))\n",
    "                        .otherwise(F.size(\"reject_reasons\")))\n",
    "           .withColumn(\"reject_reason\",\n",
    "                       F.when(F.col(\"reject_reasons\").isNull(), F.lit(None).cast(\"string\"))\n",
    "                        .otherwise(F.array_join(\"reject_reasons\", \"|\")))\n",
    "           .withColumn(\"reject_timestamp\", F.current_timestamp())\n",
    "          )\n",
    "\n",
    "    # 4) Separa válidos x rejeitos (cada registro entra UMA vez na rejects)\n",
    "#    df_reject = dfq.filter(F.col(\"reject_reason_count\") > 0 )\n",
    "#    df_valid  = dfq.filter(F.col(\"reject_reason_count\") == 0).drop(\"reject_reasons\",\"reject_reason\",\"reject_reason_count\",\"reject_timestamp\")\n",
    "    #display(dfq)\n",
    "    df_reject = dfq.filter(F.col(\"reject_reason\").isNotNull())\n",
    "    #display(df_reject)\n",
    "    df_valid  = dfq.filter(F.col(\"reject_reason\").isNull()).drop(\"reject_reasons\",\"reject_reason\",\"reject_reason_count\",\"reject_timestamp\")\n",
    "\n",
    "    # 5) Persiste rejects (append) — sem duplicação da mesma linha\n",
    "    #    Obs: não precisa mergeSchema aqui porque já criamos a tabela com colunas extras.\n",
    "    if df_reject:  # evita ação pesada; é um job pequeno\n",
    "        (df_reject\n",
    "        .write\n",
    "        .format(\"delta\")        \n",
    "        .mode(\"append\")\n",
    "        #.option(\"checkpointLocation\", \"/mnt/checkpoints/rejects\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .saveAsTable(REJECT_TABLE)\n",
    "        #.start()\n",
    "        )\n",
    "        print(f\"⚠️ {df_reject.count()} registros rejeitados salvos em {REJECT_TABLE}\")\n",
    "\n",
    "    return df_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c3c2b1-d7ab-4990-8b8d-690c0d2a8008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Compose (aplica tudo que você mostrou) -----------------------------------\n",
    "\n",
    "def apply_all_silver_calcs(df: DataFrame,\n",
    "                           *,\n",
    "                           type_col: str = \"type\",\n",
    "                           distance_col: str = \"distance\",\n",
    "                           average_speed_col: str = \"average_speed\",\n",
    "                           moving_time_col: str = \"moving_time\",\n",
    "                           elapsed_time_col: str = \"elapsed_time\",\n",
    "                           start_date_col: str = \"start_date\",\n",
    "                           non_run_value_for_pace=0  # para manter igual ao seu snippet\n",
    "                           ) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica TODAS as transformações do snippet original.\n",
    "    Retorna um novo DataFrame com:\n",
    "      - start_date (date)\n",
    "      - distance_km\n",
    "      - average_speed_kmh\n",
    "      - pace_min_km\n",
    "      - pace_min_km_moving_time\n",
    "      - tempo_real (HH:MM:SS)\n",
    "      - pace_min_km_new\n",
    "      - pace_strava (M:SS)\n",
    "      - dia_semana\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .transform(lambda d: add_start_date(d, src_col=start_date_col, out_col=\"start_date\"))\n",
    "        .transform(lambda d: add_distance_km(d, distance_col=distance_col, out_col=\"distance_km\", decimals=2))\n",
    "        .transform(lambda d: add_average_speed_kmh(d, avg_speed_col=average_speed_col, out_col=\"average_speed_kmh\", decimals=2))\n",
    "        .transform(lambda d: add_pace_min_km(d,\n",
    "                                             elapsed_time_col=elapsed_time_col,\n",
    "                                             distance_col=distance_col,\n",
    "                                             type_col=type_col,\n",
    "                                             out_col=\"pace_min_km\",\n",
    "                                             decimals=2,\n",
    "                                             only_for_run=True,\n",
    "                                             non_run_value=non_run_value_for_pace))\n",
    "        .transform(lambda d: add_pace_min_km_moving_time(d,\n",
    "                                                         moving_time_col=moving_time_col,\n",
    "                                                         distance_col=distance_col,\n",
    "                                                         type_col=type_col,\n",
    "                                                         out_col=\"pace_min_km_moving_time\",\n",
    "                                                         decimals=2,\n",
    "                                                         only_for_run=True,\n",
    "                                                         non_run_value=non_run_value_for_pace))\n",
    "        .transform(lambda d: add_tempo_real(d, seconds_col=moving_time_col, out_col=\"tempo_real\"))\n",
    "        .transform(lambda d: add_pace_min_km_new(d,\n",
    "                                                 moving_time_col=moving_time_col,\n",
    "                                                 distance_col=distance_col,\n",
    "                                                 out_col=\"pace_min_km_new\",\n",
    "                                                 decimals=3))\n",
    "        .transform(lambda d: add_pace_strava(d, pace_min_col=\"pace_min_km_new\", out_col=\"pace_strava\"))\n",
    "        .transform(lambda d: add_dia_semana(d, date_col=\"start_date\", out_col=\"dia_semana\", pattern=\"E\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0982f0f3-09db-4922-9ea5-8f0ad0b0c414",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsert_data(microBatchDF, batch):\n",
    "    microBatchDF.createOrReplaceTempView(\"activities_microbatch\")\n",
    "    \n",
    "    sql_query = \"\"\"\n",
    "                MERGE INTO uc_athlete_data.silver.strava_activities A\n",
    "                USING activities_microbatch B\n",
    "                ON A.ID = b.ID AND A.INGESTION_TIMESTAMP = B.INGESTION_TIMESTAMP\n",
    "                WHEN NOT MATCHED THEN INSERT * \n",
    "                \"\"\"  \n",
    "\n",
    "    microBatchDF.sparkSession.sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584c0261-6d2c-4d5b-83d7-b1bdd988d075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_data(microBatchDF, batch):   \n",
    "    \n",
    "    microBatchDF = dedupe_microbatch(microBatchDF,BUSINESS_KEYS,ORDER_COLS)\n",
    "    print(\"dedup ok \")\n",
    "    microBatchDF = apply_all_silver_calcs(microBatchDF)\n",
    "    print(\"calcs ok \")\n",
    "    df_clean = assert_quality(microBatchDF)\n",
    "    print(\"clean ok \")\n",
    "    upsert_data(df_clean, batch)\n",
    "    print(\"upsert ok \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07cd733-8766-46fb-a5ac-5153b1c3e742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_stream = spark.readStream.table(BRONZE)\n",
    "\n",
    "query = (bronze_stream.writeStream\n",
    "                 .foreachBatch(load_data)\n",
    "                 .option(\"checkpointLocation\", CHECKPOINT)\n",
    "                 .trigger(availableNow=True)\n",
    "                 .start())\n",
    "                 \n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "115169c7-36cf-440c-82c6-7e97c2f09dc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*) as total, reject_reason, date(reject_timestamp) as dt\n",
    "FROM uc_athlete_data.silver_rejects.strava_activities\n",
    "GROUP BY 2,3\n",
    "ORDER BY 3 DESC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "011d9c1f-2030-4568-bfbb-e03477abebb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from uc_athlete_data.silver.strava_activities\n",
    "order by ingestion_timestamp desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37eb867e-e35c-4b5d-a308-9fc1dc401fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from uc_athlete_data.silver_rejects.strava_activities\n",
    "order by reject_timestamp desc"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6095363681318923,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Stream",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
