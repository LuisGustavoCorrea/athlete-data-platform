{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d67c098-2392-439f-800b-e99c95771b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "from delta.tables import DeltaTable\n",
    "from py_functions_silver import *\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "BRONZE = \"uc_athlete_data.bronze.strava_activities\"\n",
    "SILVER = \"uc_athlete_data.silver.strava_activities\"\n",
    "CHECKPOINT = \"abfss://silver@adlsathlete.dfs.core.windows.net/strava/activities/activities_checkpoint/\"\n",
    "\n",
    "# chave(s) de negócio — mude p/ composto se precisar, ex.: [\"activity_id\",\"lap_index\"]\n",
    "BUSINESS_KEYS = [\"id\"]\n",
    "# colunas de ordenação para decidir o “vencedor” nos duplicados do micro-lote\n",
    "ORDER_COLS = [\"ingestion_timestamp\"]  # adicione \"updated_at\" se existir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd7c8014-0e43-497c-abb7-a111a5dd5ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "REJECT_TABLE = \"uc_athlete_data.silver_rejects.strava_activities\"\n",
    "\n",
    "def assert_quality(df):\n",
    "    # 1) Defina as regras (nome -> condição SQL)\n",
    "    rules = {\n",
    "        # obrigatórios\n",
    "        \"missing_id\":               \"id IS NULL\",\n",
    "        \"missing_start_date\":       \"start_date IS NULL\",\n",
    "        \"sport_type_missing\":       \"sport_type IS NULL OR sport_type = ''\",\n",
    "\n",
    "        # distância/tempo básicos\n",
    "        \"neg_distance\":             \"distance_km < 0\",\n",
    "        \"zero_distance_run\":        \"sport_type = 'Run' AND distance_km = 0\",\n",
    "        #\"nonpositive_times\":        \"elapsed_time <= 0 OR moving_time <= 0\",\n",
    "        #\"moving_gt_elapsed\":        \"moving_time > elapsed_time\",\n",
    "\n",
    "        # velocidade\n",
    "        \"speed_negative\":           \"average_speed_kmh < 0\",\n",
    "        \"speed_implausible_run\":    \"sport_type = 'Run' AND average_speed_kmh > 30\",\n",
    "        #\"speed_implausible_ride\":   \"sport_type IN ('Ride','VirtualRide') AND average_speed_kmh > 80\",\n",
    "\n",
    "        # coerência vel = dist / tempo (tolerância ~1 km/h)\n",
    "        #\"speed_inconsistent\":       \"moving_time > 0 AND distance_km >= 0 \"\n",
    "        #                            \"AND abs(average_speed_kmh - (distance_km / (moving_time/3600.0))) > 1.0\",\n",
    "\n",
    "        # pace calculado\n",
    "        \"pace_negative\":            \"pace_min_km_new < 0\",\n",
    "        \"pace_null_when_should\":    \"moving_time > 0 AND distance_km > 0 AND pace_min_km_new IS NULL\",\n",
    "\n",
    "        # elevação\n",
    "        #\"gain_negative\":            \"total_elevation_gain < 0\",\n",
    "\n",
    "        # vitais / cadência (se presentes)\n",
    "        #\"hr_out_of_range\":          \"average_heartrate IS NOT NULL AND (average_heartrate < 30 OR average_heartrate > 230)\",\n",
    "        #\"cadence_out_of_range\":     \"average_cadence IS NOT NULL AND (average_cadence < 0 OR average_cadence > 300)\",\n",
    "    }\n",
    "\n",
    "    # 2) Constrói uma coluna array com TODOS os motivos que baterem\n",
    "    #    (evita múltiplos scans/union e duplicaçāo na rejects)\n",
    "    #dfq = df.withColumn(\"reject_reasons\", F.array())  # array<string> inicial\n",
    "    dfq = df.withColumn(\"reject_reasons\", F.lit(None).cast(\"array<string>\"))\n",
    "    \n",
    "    for name, sql_cond in rules.items():\n",
    "        cond_col = F.expr(sql_cond)\n",
    "        dfq = dfq.withColumn(\n",
    "            \"reject_reasons\",\n",
    "            F.when(\n",
    "                cond_col,\n",
    "                # coalesce para lidar com NULL -> vira [] antes de unir com [name]\n",
    "                F.array_union(F.coalesce(F.col(\"reject_reasons\"), F.array()), F.array(F.lit(name)))\n",
    "            ).otherwise(F.col(\"reject_reasons\"))\n",
    "        )\n",
    "\n",
    "    # 3) Flag, contador e string consolidada (para leitura humana)\n",
    "    dfq = (dfq\n",
    "           .withColumn(\"reject_reason_count\",\n",
    "                       F.when(F.col(\"reject_reasons\").isNull(), F.lit(0))\n",
    "                        .otherwise(F.size(\"reject_reasons\")))\n",
    "           .withColumn(\"reject_reason\",\n",
    "                       F.when(F.col(\"reject_reasons\").isNull(), F.lit(None).cast(\"string\"))\n",
    "                        .otherwise(F.array_join(\"reject_reasons\", \"|\")))\n",
    "           .withColumn(\"reject_timestamp\", F.current_timestamp())\n",
    "          )\n",
    "\n",
    "    # 4) Separa válidos x rejeitos (cada registro entra UMA vez na rejects)\n",
    "#    df_reject = dfq.filter(F.col(\"reject_reason_count\") > 0 )\n",
    "#    df_valid  = dfq.filter(F.col(\"reject_reason_count\") == 0).drop(\"reject_reasons\",\"reject_reason\",\"reject_reason_count\",\"reject_timestamp\")\n",
    "    #display(dfq)\n",
    "    df_reject = dfq.filter(F.col(\"reject_reason\").isNotNull())\n",
    "    #display(df_reject)\n",
    "    df_valid  = dfq.filter(F.col(\"reject_reason\").isNull()).drop(\"reject_reasons\",\"reject_reason\",\"reject_reason_count\",\"reject_timestamp\")\n",
    "\n",
    "    # 5) Persiste rejects (append) — sem duplicação da mesma linha\n",
    "    #    Obs: não precisa mergeSchema aqui porque já criamos a tabela com colunas extras.\n",
    "    if df_reject:  # evita ação pesada; é um job pequeno\n",
    "        (df_reject\n",
    "        .write\n",
    "        .format(\"delta\")        \n",
    "        .mode(\"append\")\n",
    "        #.option(\"checkpointLocation\", \"/mnt/checkpoints/rejects\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .saveAsTable(REJECT_TABLE)\n",
    "        #.start()\n",
    "        )\n",
    "        print(f\"⚠️ {df_reject.count()} registros rejeitados salvos em {REJECT_TABLE}\")\n",
    "\n",
    "    return df_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a02eb0e-0888-4bd9-95b7-fe0536171e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _dedupe_microbatch(df):\n",
    "    w = Window.partitionBy(*[F.col(c) for c in BUSINESS_KEYS]) \\\n",
    "         .orderBy(*[F.col(c).desc() for c in ORDER_COLS])\n",
    "    return (df.withColumn(\"row_id\", F.row_number().over(w))\n",
    "              .filter(F.col(\"row_id\")==1)\n",
    "              .drop(\"row_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c3c2b1-d7ab-4990-8b8d-690c0d2a8008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Compose (aplica tudo que você mostrou) -----------------------------------\n",
    "\n",
    "def apply_all_silver_calcs(df: DataFrame,\n",
    "                           *,\n",
    "                           type_col: str = \"type\",\n",
    "                           distance_col: str = \"distance\",\n",
    "                           average_speed_col: str = \"average_speed\",\n",
    "                           moving_time_col: str = \"moving_time\",\n",
    "                           elapsed_time_col: str = \"elapsed_time\",\n",
    "                           start_date_col: str = \"start_date\",\n",
    "                           non_run_value_for_pace=0  # para manter igual ao seu snippet\n",
    "                           ) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica TODAS as transformações do snippet original.\n",
    "    Retorna um novo DataFrame com:\n",
    "      - start_date (date)\n",
    "      - distance_km\n",
    "      - average_speed_kmh\n",
    "      - pace_min_km\n",
    "      - pace_min_km_moving_time\n",
    "      - tempo_real (HH:MM:SS)\n",
    "      - pace_min_km_new\n",
    "      - pace_strava (M:SS)\n",
    "      - dia_semana\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .transform(lambda d: add_start_date(d, src_col=start_date_col, out_col=\"start_date\"))\n",
    "        .transform(lambda d: add_distance_km(d, distance_col=distance_col, out_col=\"distance_km\", decimals=2))\n",
    "        .transform(lambda d: add_average_speed_kmh(d, avg_speed_col=average_speed_col, out_col=\"average_speed_kmh\", decimals=2))\n",
    "        .transform(lambda d: add_pace_min_km(d,\n",
    "                                             elapsed_time_col=elapsed_time_col,\n",
    "                                             distance_col=distance_col,\n",
    "                                             type_col=type_col,\n",
    "                                             out_col=\"pace_min_km\",\n",
    "                                             decimals=2,\n",
    "                                             only_for_run=True,\n",
    "                                             non_run_value=non_run_value_for_pace))\n",
    "        .transform(lambda d: add_pace_min_km_moving_time(d,\n",
    "                                                         moving_time_col=moving_time_col,\n",
    "                                                         distance_col=distance_col,\n",
    "                                                         type_col=type_col,\n",
    "                                                         out_col=\"pace_min_km_moving_time\",\n",
    "                                                         decimals=2,\n",
    "                                                         only_for_run=True,\n",
    "                                                         non_run_value=non_run_value_for_pace))\n",
    "        .transform(lambda d: add_tempo_real(d, seconds_col=moving_time_col, out_col=\"tempo_real\"))\n",
    "        .transform(lambda d: add_pace_min_km_new(d,\n",
    "                                                 moving_time_col=moving_time_col,\n",
    "                                                 distance_col=distance_col,\n",
    "                                                 out_col=\"pace_min_km_new\",\n",
    "                                                 decimals=3))\n",
    "        .transform(lambda d: add_pace_strava(d, pace_min_col=\"pace_min_km_new\", out_col=\"pace_strava\"))\n",
    "        .transform(lambda d: add_dia_semana(d, date_col=\"start_date\", out_col=\"dia_semana\", pattern=\"E\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "115169c7-36cf-440c-82c6-7e97c2f09dc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*) as total, reject_reason, date(reject_timestamp) as dt\n",
    "FROM uc_athlete_data.silver_rejects.strava_activities\n",
    "GROUP BY 2,3\n",
    "ORDER BY 3 DESC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0982f0f3-09db-4922-9ea5-8f0ad0b0c414",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsert_data(microBatchDF, batch):\n",
    "    microBatchDF.createOrReplaceTempView(\"activities_microbatch\")\n",
    "    \n",
    "    sql_query = \"\"\"\n",
    "                MERGE INTO uc_athlete_data.silver.strava_activities A\n",
    "                USING activities_microbatch B\n",
    "                ON A.ID = b.ID AND A.INGESTION_TIMESTAMP = B.INGESTION_TIMESTAMP\n",
    "                WHEN NOT MATCHED THEN INSERT * \n",
    "                \"\"\"  \n",
    "\n",
    "    microBatchDF.sparkSession.sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584c0261-6d2c-4d5b-83d7-b1bdd988d075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_data(microBatchDF, batch):   \n",
    "    \n",
    "    microBatchDF = _dedupe_microbatch(microBatchDF)\n",
    "    print(\"dedup ok \")\n",
    "    microBatchDF = apply_all_silver_calcs(microBatchDF)\n",
    "    print(\"calcs ok \")\n",
    "    df_clean = assert_quality(microBatchDF)\n",
    "    print(\"clean ok \")\n",
    "    upsert_data(df_clean, batch)\n",
    "    print(\"upsert ok \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07cd733-8766-46fb-a5ac-5153b1c3e742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_stream = spark.readStream.table(BRONZE)\n",
    "query = (bronze_stream.writeStream\n",
    "                 .foreachBatch(load_data)\n",
    "                 .option(\"checkpointLocation\", CHECKPOINT)\n",
    "                 .trigger(availableNow=True)\n",
    "                 .start())\n",
    "                 \n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5294823549023526,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Stream",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
