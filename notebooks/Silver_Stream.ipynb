{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d67c098-2392-439f-800b-e99c95771b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "from delta.tables import DeltaTable\n",
    "from py_functions_silver import *\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "BRONZE = \"uc_athlete_data.bronze.strava_activities\"\n",
    "SILVER = \"uc_athlete_data.silver.strava_activities\"\n",
    "CHECKPOINT = \"abfss://silver@adlsathlete.dfs.core.windows.net/strava/activities/activities_checkpoint/\"\n",
    "\n",
    "# chave(s) de negócio — mude p/ composto se precisar, ex.: [\"activity_id\",\"lap_index\"]\n",
    "BUSINESS_KEYS = [\"id\"]\n",
    "# colunas de ordenação para decidir o “vencedor” nos duplicados do micro-lote\n",
    "ORDER_COLS = [\"ingestion_timestamp\"]  # adicione \"updated_at\" se existir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e5d3b4-c1f1-4767-b40e-bad38e770927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- mesma estrutura, zero linhas\n",
    "CREATE TABLE uc_athlete_data.silver.stage_strava_activities\n",
    "AS\n",
    "SELECT *\n",
    "FROM uc_athlete_data.silver.strava_activities\n",
    "WHERE 1 = 0;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87695d2e-9784-4e50-8ce2-1b1743250ec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_stream = spark.readStream.table(BRONZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a02eb0e-0888-4bd9-95b7-fe0536171e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _dedupe_microbatch(df):\n",
    "    w = Window.partitionBy(*[F.col(c) for c in BUSINESS_KEYS]) \\\n",
    "         .orderBy(*[F.col(c).desc() for c in ORDER_COLS])\n",
    "    return (df.withColumn(\"row_id\", F.row_number().over(w))\n",
    "              .filter(F.col(\"row_id\")==1)\n",
    "              .drop(\"row_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c3c2b1-d7ab-4990-8b8d-690c0d2a8008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Compose (aplica tudo que você mostrou) -----------------------------------\n",
    "\n",
    "def apply_all_silver_calcs(df: DataFrame,\n",
    "                           *,\n",
    "                           type_col: str = \"type\",\n",
    "                           distance_col: str = \"distance\",\n",
    "                           average_speed_col: str = \"average_speed\",\n",
    "                           moving_time_col: str = \"moving_time\",\n",
    "                           elapsed_time_col: str = \"elapsed_time\",\n",
    "                           start_date_col: str = \"start_date\",\n",
    "                           non_run_value_for_pace=0  # para manter igual ao seu snippet\n",
    "                           ) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica TODAS as transformações do snippet original.\n",
    "    Retorna um novo DataFrame com:\n",
    "      - start_date (date)\n",
    "      - distance_km\n",
    "      - average_speed_kmh\n",
    "      - pace_min_km\n",
    "      - pace_min_km_moving_time\n",
    "      - tempo_real (HH:MM:SS)\n",
    "      - pace_min_km_new\n",
    "      - pace_strava (M:SS)\n",
    "      - dia_semana\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .transform(lambda d: add_start_date(d, src_col=start_date_col, out_col=\"start_date\"))\n",
    "        .transform(lambda d: add_distance_km(d, distance_col=distance_col, out_col=\"distance_km\", decimals=2))\n",
    "        .transform(lambda d: add_average_speed_kmh(d, avg_speed_col=average_speed_col, out_col=\"average_speed_kmh\", decimals=2))\n",
    "        .transform(lambda d: add_pace_min_km(d,\n",
    "                                             elapsed_time_col=elapsed_time_col,\n",
    "                                             distance_col=distance_col,\n",
    "                                             type_col=type_col,\n",
    "                                             out_col=\"pace_min_km\",\n",
    "                                             decimals=2,\n",
    "                                             only_for_run=True,\n",
    "                                             non_run_value=non_run_value_for_pace))\n",
    "        .transform(lambda d: add_pace_min_km_moving_time(d,\n",
    "                                                         moving_time_col=moving_time_col,\n",
    "                                                         distance_col=distance_col,\n",
    "                                                         type_col=type_col,\n",
    "                                                         out_col=\"pace_min_km_moving_time\",\n",
    "                                                         decimals=2,\n",
    "                                                         only_for_run=True,\n",
    "                                                         non_run_value=non_run_value_for_pace))\n",
    "        .transform(lambda d: add_tempo_real(d, seconds_col=moving_time_col, out_col=\"tempo_real\"))\n",
    "        .transform(lambda d: add_pace_min_km_new(d,\n",
    "                                                 moving_time_col=moving_time_col,\n",
    "                                                 distance_col=distance_col,\n",
    "                                                 out_col=\"pace_min_km_new\",\n",
    "                                                 decimals=3))\n",
    "        .transform(lambda d: add_pace_strava(d, pace_min_col=\"pace_min_km_new\", out_col=\"pace_strava\"))\n",
    "        .transform(lambda d: add_dia_semana(d, date_col=\"start_date\", out_col=\"dia_semana\", pattern=\"E\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37308d81-c704-41e3-849c-f0c2a2826384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datacontract.data_contract import DataContract\n",
    "\n",
    "def validate_df_with_contract(df, contract_path, temp_view_name):\n",
    "    view_name = \"strava_activities\"        # precisa bater com o nome do modelo no YAML\n",
    "    (df\n",
    "     .select(\"id\",\"start_date\",\"distance\",\"pace\",\"week_day\")\n",
    "     .createOrReplaceTempView(view_name))\n",
    "\n",
    "\n",
    "    #df.createOrReplaceTempView(temp_view_name)  # deve ter o mesmo nome do modelo no YAML\n",
    "    dc = DataContract(data_contract_file=contract_path, spark=spark)\n",
    "    \n",
    "    run = dc.test()\n",
    "\n",
    "    \n",
    "    print(run.pretty())             # resumo legível (pass/fail por check)\n",
    "    print(\"PASSOU?\", run.has_passed())\n",
    "\n",
    "    if not run.has_passed():\n",
    "        # opcional: print(run.result) ou exportar JUnit no CI\n",
    "        raise Exception(\"Data contract failed for micro-batch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0982f0f3-09db-4922-9ea5-8f0ad0b0c414",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsert_data(microBatchDF, batch):\n",
    "    microBatchDF = _dedupe_microbatch(microBatchDF)\n",
    "    microBatchDF = apply_all_silver_calcs(microBatchDF)\n",
    "    #microBatchDF = add_silver_ingestion(microBatchDF)\n",
    "\n",
    "        # valida o micro-lote (pré-write)\n",
    "    run = validate_df_with_contract(\n",
    "        microBatchDF.select(\"id\",\"start_date\",\"distance_km\",\"pace_min_km\",\"dia_semana\"),\n",
    "        \"/Workspace/Users/lgcpazdb892@outlook.com/EngineData/athlete-data-platform/notebooks/dataContract.yaml\",\n",
    "        \"dataframe\"\n",
    "    )\n",
    "\n",
    "    microBatchDF.createOrReplaceTempView(\"activities_microbatch\")\n",
    "    \n",
    "    sql_query = \"\"\"\n",
    "                MERGE INTO uc_athlete_data.silver.stage_strava_activities A\n",
    "                USING activities_microbatch B\n",
    "                ON A.ID = b.ID AND A.INGESTION_TIMESTAMP = B.INGESTION_TIMESTAMP\n",
    "                WHEN NOT MATCHED THEN INSERT * \n",
    "                \"\"\"  \n",
    "\n",
    "    #microBatchDF.sparkSession.sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07cd733-8766-46fb-a5ac-5153b1c3e742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "query = (bronze_stream.writeStream\n",
    "                 .foreachBatch(upsert_data)\n",
    "                 .option(\"checkpointLocation\", CHECKPOINT)\n",
    "                 .trigger(availableNow=True)\n",
    "                 .start())\n",
    "                 \n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7840c4ad-64cb-4507-8f17-dc144abd1086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from uc_athlete_data.silver.stage_strava_activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c65fd15f-2880-412f-b100-b677b3e6d096",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759779654430}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from uc_athlete_data.silver.strava_activities WHERE id in (14165154538,60602,60601,5050,50501);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7744b8ab-a827-45f9-992d-9c8714adcd80",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"start_date\":196},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759781895358}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from uc_athlete_data.bronze.strava_activities  WHERE id in (14165154538,60603)\n",
    "order by id asc;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d12d6195-aa88-48f5-add2-08e6d816d585",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"operationMetrics\":1500},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758669190969}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "describe history uc_athlete_data.silver.stage_silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13516cbf-bcc1-4fb1-b963-c3ff0db0a5a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "def upsert_data(microBatchDF, batch_id):\n",
    "    delta_table = DeltaTable.forName(\n",
    "        microBatchDF.sparkSession,\n",
    "        \"uc_athlete_data.silver.stage_silver\"\n",
    "    )\n",
    "    (\n",
    "        delta_table.alias(\"A\")\n",
    "        .merge(\n",
    "            microBatchDF.alias(\"B\"),\n",
    "            \"A.ID = B.ID AND A.INGESTION_TIMESTAMP = B.INGESTION_TIMESTAMP\"\n",
    "        )\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bf443c7-07ed-4ce4-bb03-38a9dff600ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install datacontract-cli[databricks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62f77093-a49d-400f-9cf4-f2d53ee23e70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb26cc3e-5ff9-4fa3-b593-800c393ff012",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad85984-188a-4032-980e-4c4f202a7a74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_stream = spark.readStream.table(SILVER)\n",
    "bronze_stream.createOrReplaceTempView(\"strava_activities\")\n",
    "\n",
    "def validate_with_contract_df(df, contract_path, model_view=\"strava_activities\"):\n",
    "    df.createOrReplaceTempView(model_view)  # nome deve bater com o modelo no YAML\n",
    "    dc = DataContract(data_contract_file=contract_path, spark=spark)\n",
    "    run = dc.test(server=\"production\")\n",
    "    print(run.pretty())\n",
    "    if not run.has_passed():\n",
    "        raise Exception(\"Data contract (DF) reprovado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "467171b2-678f-4134-a42d-f11dc64337d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5432438c-4660-4754-ba05-b40f04a17acb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O código abaixo roda um datacontract na silver delta. esta funcionando. temos a lógica feita - Proximo passo: encaixar no processo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d77cab7-f514-4a5b-8374-a5de354ea319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datacontract.data_contract import DataContract\n",
    "\n",
    "# defina a tabela que você quer usar no teste:\n",
    "SILVER = \"uc_athlete_data.silver.strava_activities\"  # ajuste se for outra\n",
    "\n",
    "# leia em batch (não streaming)\n",
    "df = spark.read.table(SILVER)\n",
    "\n",
    "def validate_with_contract_df(df, contract_path, model_view=\"strava_activities\"):\n",
    "    # dica: selecione só as colunas definidas no YAML pra evitar schema mismatch\n",
    "    # cols = [\"id\",\"athlete_id\",\"start_date\",\"start_date_local\",\"timezone\",\"sport_type\",\n",
    "    #         \"elapsed_time\",\"moving_time\",\"distance_km\",\"average_speed_kmh\",\"total_elevation_gain\",\n",
    "    #         \"has_heartrate\",\"average_heartrate\",\"max_heartrate\",\"average_cadence\",\"week_day\",\n",
    "    #         \"ingestion_timestamp\"]\n",
    "    # df = df.select(*[c for c in cols if c in df.columns])\n",
    "\n",
    "    df.createOrReplaceTempView(model_view)  # precisa bater com o nome do modelo no YAML\n",
    "    dc = DataContract(data_contract_file=contract_path, spark=spark)\n",
    "    run = dc.test()\n",
    "    print(run.pretty())\n",
    "    if not run.has_passed():\n",
    "        raise Exception(\"Data contract (DF) reprovado\")\n",
    "\n",
    "# use:\n",
    "validate_with_contract_df(\n",
    "    df,\n",
    "    \"/Workspace/Users/lgcpazdb892@outlook.com/EngineData/athlete-data-platform/notebooks/dataContract.yaml\",\n",
    "    model_view=\"strava_activities\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb7636a7-29e2-497b-9177-3d0b51c81f5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Configs -----------------------------\n",
    "CONFIG = {\n",
    "    \"ENABLE_PRE_WRITE_CONTRACT\":  True,   # valida DF (temp view) antes do write\n",
    "    \"ENABLE_POST_WRITE_CONTRACT\": True,   # valida a tabela após o write\n",
    "    \"FAIL_ON_REJECTS\":            False,  # se True: lote falha quando houver rejeitos\n",
    "    \"ENFORCEMENT_MODE\":           \"fail\", # \"fail\" (derruba) ou \"warn\" (apenas log)\n",
    "    \"MODEL_VIEW_NAME\":            \"strava_activities\",  # deve bater com models.<nome> no YAML\n",
    "    \"CONTRACT_DF_PATH\":           \"/Workspace/Repos/.../contracts/strava_activities_dataframe.yaml\",\n",
    "    \"CONTRACT_TABLE_PATH\":        \"/Workspace/Repos/.../contracts/strava_activities_table.yaml\",\n",
    "    \"SILVER_TABLE\":               \"uc_athlete_data.silver.strava_activities\",\n",
    "    \"REJECTS_TABLE\":              \"uc_athlete_data.silver_rejects.strava_activities\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61ebf0be-de9c-4522-95ba-50fb13735e47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from datacontract.data_contract import DataContract\n",
    "\n",
    "# ---------------- Enforcement ----------------\n",
    "def enforce_or_warn(condition: bool, message: str, cfg=CONFIG):\n",
    "    \"\"\"Se ENFORCEMENT_MODE='fail' e condition=False => raise. Caso contrário, loga warn.\"\"\"\n",
    "    if condition:\n",
    "        return\n",
    "    if cfg[\"ENFORCEMENT_MODE\"].lower() == \"fail\":\n",
    "        raise Exception(message)\n",
    "    else:\n",
    "        print(f\"[WARN] {message}\")\n",
    "\n",
    "# ---------------- Contracts -------------------\n",
    "CONTRACT_COLS = [\"id\",\"athlete_id\",\"start_date\",\"sport_type\",\n",
    "                 \"elapsed_time\",\"moving_time\",\"distance_km\",\n",
    "                 \"average_speed_kmh\",\"dia_semana\"]\n",
    "\n",
    "def validate_with_contract_df(df, cfg=CONFIG):\n",
    "    if not cfg[\"ENABLE_PRE_WRITE_CONTRACT\"]:\n",
    "        print(\"[INFO] Pré-write contract desabilitado.\")\n",
    "        return\n",
    "    view = cfg[\"MODEL_VIEW_NAME\"]\n",
    "    # seleciona apenas colunas esperadas (evita mismatch)\n",
    "    cols = [c for c in CONTRACT_COLS if c in df.columns]\n",
    "    df.select(*cols).createOrReplaceTempView(view)\n",
    "\n",
    "    dc = DataContract(data_contract_file=cfg[\"CONTRACT_DF_PATH\"], spark=spark)\n",
    "    run = dc.test(server=\"production\")\n",
    "    print(run.pretty())\n",
    "    enforce_or_warn(run.has_passed(), \"Data contract (DF) reprovado\", cfg)\n",
    "\n",
    "def validate_with_contract_table(cfg=CONFIG):\n",
    "    if not cfg[\"ENABLE_POST_WRITE_CONTRACT\"]:\n",
    "        print(\"[INFO] Pós-write contract desabilitado.\")\n",
    "        return\n",
    "    dc = DataContract(data_contract_file=cfg[\"CONTRACT_TABLE_PATH\"], spark=spark)\n",
    "    run = dc.test(server=\"production\")\n",
    "    print(run.pretty())\n",
    "    enforce_or_warn(run.has_passed(), \"Data contract (Tabela) reprovado\", cfg)\n",
    "\n",
    "# ---------------- Split good / rejects --------\n",
    "REQUIRED = [\"id\",\"athlete_id\",\"start_date\",\"sport_type\"]\n",
    "NUM_MIN0 = [\"elapsed_time\",\"moving_time\",\"distance_km\",\"average_speed_kmh\"]\n",
    "\n",
    "def cast_and_validate(df, cfg=CONFIG):\n",
    "    # Tipagem forte\n",
    "    df = (df\n",
    "          .withColumn(\"id\", F.col(\"id\").cast(T.LongType()))\n",
    "          .withColumn(\"athlete_id\", F.col(\"athlete_id\").cast(T.LongType()))\n",
    "          .withColumn(\"start_date\", F.to_timestamp(\"start_date\"))\n",
    "          .withColumn(\"elapsed_time\", F.col(\"elapsed_time\").cast(T.LongType()))\n",
    "          .withColumn(\"moving_time\", F.col(\"moving_time\").cast(T.LongType()))\n",
    "          .withColumn(\"distance_km\", F.col(\"distance_km\").cast(T.DoubleType()))\n",
    "          .withColumn(\"average_speed_kmh\", F.col(\"average_speed_kmh\").cast(T.DoubleType()))\n",
    "          .withColumn(\"dia_semana\", F.col(\"dia_semana\").cast(T.StringType()))\n",
    "         )\n",
    "\n",
    "    # Regras base\n",
    "    cond = F.lit(True)\n",
    "    for c in REQUIRED:  cond = cond & F.col(c).isNotNull()\n",
    "    for c in NUM_MIN0:  cond = cond & (F.col(c).isNull() | (F.col(c) >= 0))\n",
    "    cond = cond & (F.col(\"moving_time\").isNull() | F.col(\"elapsed_time\").isNull() | (F.col(\"moving_time\") <= F.col(\"elapsed_time\")))\n",
    "    cond = cond & (F.col(\"dia_semana\").isNull())\n",
    "\n",
    "    good = df.where(cond)\n",
    "    bad  = (df.where(~cond)\n",
    "              .withColumn(\"reject_reason\",\n",
    "                  F.when(F.col(\"id\").isNull(), \"id_null\")\n",
    "                   .when(F.col(\"athlete_id\").isNull(), \"athlete_id_null\")\n",
    "                   .when(F.col(\"start_date\").isNull(), \"start_date_null\")\n",
    "                   .when(F.col(\"sport_type\").isNull(), \"sport_type_null\")\n",
    "                   .when(F.col(\"elapsed_time\") < 0, \"elapsed_time_neg\")\n",
    "                   .when(F.col(\"moving_time\") < 0, \"moving_time_neg\")\n",
    "                   .when(F.col(\"moving_time\") > F.col(\"elapsed_time\"), \"moving>elapsed\")\n",
    "                   .when((F.col(\"dia_semana\").isNull()), \"dia_semana_null\")\n",
    "                   .otherwise(\"invalid_generic\")\n",
    "              ))\n",
    "\n",
    "    return good, bad\n",
    "\n",
    "def handle_rejects(bad_df, cfg=CONFIG):\n",
    "    if bad_df is None or bad_df.rdd.isEmpty():\n",
    "        print(\"[INFO] Nenhum rejeito no micro-lote.\")\n",
    "        return\n",
    "    (bad_df.withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "           .write.mode(\"append\").format(\"delta\")\n",
    "           .saveAsTable(cfg[\"REJECTS_TABLE\"]))\n",
    "    print(\"[WARN] Rejeitos gravados em\", cfg[\"REJECTS_TABLE\"])\n",
    "    enforce_or_warn(not cfg[\"FAIL_ON_REJECTS\"], \"Rejeitos detectados; lote abortado\", cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c3e2326-d964-45f1-96d7-a66ffa42934c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"uc_athlete_data.silver.strava_activities\")  # ou outra origem\n",
    "\n",
    "good, bad = cast_and_validate(df, CONFIG)\n",
    "handle_rejects(bad, CONFIG)\n",
    "validate_with_contract_df(good, CONFIG)       # só roda se ENABLE_PRE_WRITE_CONTRACT=True\n",
    "# ... simule um write aqui, se quiser ...\n",
    "validate_with_contract_table(CONFIG)          # só roda se ENABLE_POST_WRITE_CONTRACT=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27ae6e96-fe85-4de1-bc53-8eec5805aaf8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759961284182}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee134cc3-bad3-4aad-bffc-514feab413c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1) Cria o schema de rejeitos (se ainda não existir)\n",
    "CREATE SCHEMA IF NOT EXISTS uc_athlete_data.silver_rejects;\n",
    "\n",
    "-- 2) Cria a tabela de rejeitos\n",
    "CREATE TABLE IF NOT EXISTS uc_athlete_data.silver_rejects.strava_activities (\n",
    "  id                   BIGINT,\n",
    "  athlete_id           BIGINT,\n",
    "  start_date           TIMESTAMP,\n",
    "  start_date_local     TIMESTAMP,\n",
    "  timezone             STRING,\n",
    "  sport_type           STRING,\n",
    "  elapsed_time         BIGINT,\n",
    "  moving_time          BIGINT,\n",
    "  distance_km          DOUBLE,\n",
    "  average_speed_kmh    DOUBLE,\n",
    "  total_elevation_gain DOUBLE,\n",
    "  has_heartrate        BOOLEAN,\n",
    "  average_heartrate    INT,\n",
    "  max_heartrate        INT,\n",
    "  average_cadence      DOUBLE,\n",
    "  dia_semana           STRING,\n",
    "  -- campos de auditoria da rejeição:\n",
    "  reject_reason        STRING,        -- motivo da rejeição (ex.: required_null, moving>elapsed, etc.)\n",
    "  ingestion_timestamp  TIMESTAMP      -- carimbo do momento que salvou na tabela de rejeitos\n",
    ")\n",
    "USING DELTA;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8783823913413670,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Stream",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
